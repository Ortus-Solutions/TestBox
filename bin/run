#!/usr/bin/env boxlang

/**
 * TestBox Runner for BoxLang
 * This script will run TestBox tests from the command line using the BoxLang CLI
 *
 * Examples:
 * 	- `./testbox/bin/run`
 * 	- `./testbox/bin/run my.bundle`
 * 	- `./testbox/bin/run --test-directory=tests.specs`
 * 	- `./testbox/bin/run --test-bundles=my.bundle`
 *
 * Options:
 * --test-bundles: A list of test bundles to run, defaults to `*`, ex: `path.to.bundle1,path.to.bundle2`, . Mutually exclusive with `--test-directory`
 * --test-bundles-pattern: A pattern to match test bundles, defaults to `"*Spec*.cfc|*Test*.cfc|*Spec*.bx|*Test*.bx"`
 * --test-directory : A list of directories to look for tests to execute.  Please use dot-notation not absolute notation.
 * 		Mutually exclusive with `--test-bundles`. Ex: `tests.specs`.  Defaults to `tests.specs`
 * --test-reporter : The reporter to use.
 * --test-labels : A list of labels to run, defaults to `*`
 * --test-excludes : A list of labels to exclude, defaults to empty
 * --test-recurse : Recurse into subdirectories, defaults to `true`
 * --test-filter-bundles : A list of bundles to filter by, defaults to `*`
 * --test-filter-suites : A list of suites to filter by, defaults to `*`
 * --test-filter-specs : A list of test names or spec names to filter by, defaults to `*`
 * --test-eager-failure : Fail fast, defaults to `false`
 * --test-runner-options: A JSON struct literal of options to pass into the test runner. Ex: `{"verbose"=true}`
 * --test-verbose : Verbose output, defaults to `false`. This will stream the output of the status of the tests as they run.
 * --test-properties-summary : Generate a properties file with the summary of the test results, defaults to `true`.
 * --test-properties-filename : The name of the properties file to generate, defaults to `TEST.properties`
 * 	If true, it will write them to the report path.
 * --test-reportpath : The path to write the report file to, defaults to the `/tests/results` folder by convention
 * --test-write-report : Write the report to a file in the report path folder, defaults to `true`
 * --test-write-json-report : Write the report as JSON alongside the requested report, defaults to `false`
 * --test-write-visualizer : Write the visualizer to a file in the report path folder, defaults to `false`
 */

function escapePropertyValue( required string value ) {
	if ( len( arguments.value ) == 0 ) {
		return arguments.value;
	}
	local.value = replaceNoCase( arguments.value, '\', '\\', 'all' );
	value = replaceNoCase( value, char(13), '\r', 'all' );
	value = replaceNoCase( value, char(10), '\n', 'all' );
	value = replaceNoCase( value, char(9), '\t', 'all' );
	value = replaceNoCase( value, char(60), '\u003c', 'all' );
	value = replaceNoCase( value, char(62), '\u003e', 'all' );
	value = replaceNoCase( value, char(47), '\u002f', 'all' );
	return replaceNoCase( value, char(32), '\u0020', 'all' );
}

// CLI variables
rootPath = server.cli.executionPath
options = server.cli.parsed.options;
positional = server.cli.parsed.positionals;

// Defaults
DEFAULT_TEST_DIRECTORY = "tests.specs"
DEFAULT_REPORTER = "text"
DEFAULT_REPORT_PATH = rootPath & "/tests/results"
DEFAULT_PROPERTIES_FILENAME = "TEST.properties"
DEFAULT_PROPERTIES_SUMMARY = true

// Gather the test arguments from the options
initArgs = {
	bundles = options[ "test-bundles" ] ?: [],
	directory = {
		mapping : options[ "test-directory" ] ?: "",
		recurse : options[ "test-recurse" ] ?: true
	},
	reporter = options[ "test-reporter" ] ?: DEFAULT_REPORTER,
	labels = options[ "test-labels" ] ?: "",
	excludes = options[ "test-excludes" ] ?: "",
	options = options[ "test-runner-options" ] ?: {},
	bundlesPattern = options[ "test-bundles-pattern" ] ?: ""
};

// Deserialize the JSON options
if( isSimpleValue( initArgs.options ) && initArgs.options.len() ) {
	initArgs.options = jsonDeserialize( initArgs.options );
}

// Prepare the run arguments
runArgs = {
	testBundles = options[ "test-filter-bundles" ] ?: [],
	testSuites = options[ "test-filter-suites" ] ?: [],
	testSpecs = options[ "test-filter-specs" ] ?: [],
	eagerFailure = options[ "test-eager-failure" ] ?: false,
	verbose = options[ "test-verbose" ] ?: false
};

// Prepare the after run arguments
afterRunArgs = {
	propertiesSummary = options[ "test-properties-summary" ] ?: DEFAULT_PROPERTIES_SUMMARY,
	propertiesFilename = options[ "test-properties-filename" ] ?: DEFAULT_PROPERTIES_FILENAME,
	reportPath = options[ "test-reportpath" ] ?: DEFAULT_REPORT_PATH,
	writeReport = options[ "test-write-report" ] ?: true,
	writeVisualizer = options[ "test-write-visualizer" ] ?: false,
	writeJsonReport = options[ "test-write-json-report" ] ?: false
};

// Verbose Listeners
if( runArgs.verbose ){
	runArgs.callbacks = {
		onBundleStart = ( target, testResults ) => {
			println( "> Testing Bundle: #target.$bx.meta.name#" )
		},
		onBundleEnd = ( target, testResults ) => {
			println( "> Bundle Completed: [#target.$bx.meta.name#]" )
			println( "" );
		},
		onSuiteStart = ( target, testResults, suite ) => {
			println( "+ Starting Suite: #suite.name#" )
		},
		onSuiteEnd = ( target, testResults, suite ) => {
			//println( "+ Suite [#suite.name#] completed #suite.toString()#" )
		},
		onSpecStart = ( target, testResults, suite, spec ) => {
			println( "+ Starting Spec/Test: #spec.name#" )
		},
		onSpecEnd = ( target, testResults, suite, spec ) => {
			// println( "+ Spec [#spec.name#] completed #spec.toString()#" )
		},
	}
}

// If we have a positional argument, then we will assume it is a test bundle: Ex: `run my.bundle`
if( positional.len() ) {
	initArgs.bundles = positional[ 1 ];
}

// If we don't have test-bundles or test-directory, then default to the DEFAULT_TEST_DIRECTORY
if( !initArgs.bundles.len() && !initArgs.directory.mapping.len() ) {
	initArgs.directory = DEFAULT_TEST_DIRECTORY;
}

if( runArgs.verbose ){
	startTime = getTickCount();
	println( "Starting TestBox Runner with the following init arguments" );
	println( initArgs );
}
testbox = new testbox.system.TestBox( argumentCollection = initArgs )
if( runArgs.verbose ){
	println( "TestBox Runner started in #getTickCount() - startTime# ms" );
	println( "Running your tests with the following run arguments" );
	println( runArgs );
} else{
	println( "Running your tests..." )
}

// RUN BABY RUN
println( "" )
report = testbox.run( argumentCollection = runArgs )
testResults = testbox.getResult()
testResultsAsJson = jsonSerialize( testResults.getMemento( includeDebugBuffer = true ) )
println( report )

// PREPARE RESULTS FOR REPORTING
if( !directoryExists( afterRunArgs.reportPath ) ){
	directoryCreate( afterRunArgs.reportPath );
} else {
	directoryDelete( afterRunArgs.reportPath, true );
	directoryCreate( afterRunArgs.reportPath );
}

// REPORTING TIME
fileWrite(
	afterRunArgs.reportPath & "/latestrun.log",
	"Tests ran at #dateTimeFormat( now(), 'medium' )#"
)

// WRITE THE REPORT
if( afterRunArgs.writeReport ){
	reportFile = afterRunArgs.reportPath & "/report."
	switch( initArgs.reporter ){
		case  "min": case "simple" :
			reportFile &= "html";
			break;
		case "json":
			reportFile &= "json";
			break;
		case "xml": case "ANTJunit":
			reportFile &= "xml";
			break;
		default:
			reportFile &= "txt";
	}
	fileWrite(
		reportFile,
		report
	)
}

// WRITE THE JSON REPORT
if( afterRunArgs.writeJsonReport ){
	fileWrite(
		afterRunArgs.reportPath & "/report.json",
		testResultsAsJson
	)
}

// WRITE THE VISUALIZER
if( afterRunArgs.writeVisualizer ){
	directoryCopy(
		expandPath( "/testbox/test-visualizer" ),
		afterRunArgs.reportPath & "/visualizer"
	)
	fileWrite(
		afterRunArgs.reportPath & "/visualizer/test-results.json",
		testResultsAsJson
	)
}

// WRITE THE SUMMARIES
if( afterRunArgs.propertiesSummary ) {
	errors = testResults.getTotalFail() + testResults.getTotalError();
	propertiesReport = "## TestBox Summary Report
test.datetime=#now().toISOString()#
test.#errors ? 'failed' : 'passed'#=true
test.labels=#escapePropertyValue( arrayToList( testResults.getLabels() ) )#
test.excludes=#escapePropertyValue( arrayToList( testResults.getExcludes() ) )#
test.bundles=#escapePropertyValue( initArgs.bundles )#
test.directory=#escapePropertyValue( initArgs.directory.mapping )#
total.bundles=#escapePropertyValue( testResults.getTotalBundles() )#
total.suites=#escapePropertyValue( testResults.getTotalSuites() )#
total.specs=#escapePropertyValue( testResults.getTotalSpecs() )#
total.pass=#escapePropertyValue( testResults.getTotalPass() )#
total.fail=#escapePropertyValue( testResults.getTotalFail() )#
total.error=#escapePropertyValue( testResults.getTotalError() )#
total.skipped=#escapePropertyValue( testResults.getTotalSkipped() )#";

	if( !trim( lcase( afterRunArgs.propertiesfilename ) ).endsWith( '.properties' ) ) {
		afterRunArgs.propertiesfilename &= '.properties';
	}

	fileWrite(
		afterRunArgs.reportPath & "/" & afterRunArgs.propertiesFilename,
		propertiesReport
	)
}

// do stupid JUnitReport task processing, if the report is ANTJunit
if( initArgs.reporter eq "ANTJunit" ){
	// Produce individual test files due to how ANT JUnit report parses these.
	xmlReport = xmlParse( report );
	for( thisSuite in xmlReport.testsuites.XMLChildren ){
		fileWrite( afterRunArgs.reportpath & "/TEST-" & thisSuite.XMLAttributes.package & ".xml", toString( thisSuite ) );
	}
}
